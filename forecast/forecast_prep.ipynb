{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nmert\\OneDrive\\Data Science\\OMSA\\HW & Projects\\omsa_dva_project\\.venv\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "c:\\Users\\nmert\\OneDrive\\Data Science\\OMSA\\HW & Projects\\omsa_dva_project\\.venv\\lib\\site-packages\\numpy\\.libs\\libopenblas.EL2C6PLE4ZYW3ECEVIV3OXXGRN2NRFM2.gfortran-win_amd64.dll\n",
      "c:\\Users\\nmert\\OneDrive\\Data Science\\OMSA\\HW & Projects\\omsa_dva_project\\.venv\\lib\\site-packages\\numpy\\.libs\\libopenblas.XWYDX2IKJW2NMTWSFYNGFUWKQU3LYTCZ.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from data_prep.prep import MeterDataSet\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from webapp.utils.azure_utils import KeyVault, DataLake\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to Storage Account\n",
    "vault = KeyVault(keyVaultName = \"keyvaultdva2022\")\n",
    "storage_credential = vault.get_secret(secretName = \"storagePrimaryKey\")\n",
    "storage = DataLake(account_name = \"storageaccountdva\", credential = storage_credential)\n",
    "file_system = \"energyhub\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "meter = \"electricity\"\n",
    "metadata_cols = ['building_id', 'site_id','sq_meter']\n",
    "weather_cols = ['site_id', 'timestamp', 'air_temperature', 'cloud_coverage', 'dew_temperature', 'precipitation_depth_1_hr',\n",
    "    'precipitation_depth_6_hr', 'sea_level_pressure', 'wind_direction',\n",
    "    'wind_speed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = storage.pandas_read(file_system, directory=\"data_parq/metadata\", file_name=\"metadata.parq\")\n",
    "weather = storage.pandas_read(file_system, directory=\"data_parq/weather\", file_name=\"weather.parq\")\n",
    "electricity = storage.pandas_read(file_system, directory=\"data_parq/meters\", file_name=\"electricity.parq\")\n",
    "bad_buildings = storage.pandas_read(file_system, directory=\"bad_buildings\", file_name=\"bad_buildings.csv\")[\"building_id\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add cos, sin time features:\n",
    "def time_features(dff):\n",
    "    dff = deepcopy(dff)\n",
    "\n",
    "    dff[\"hour\"] = dff.timestamp.dt.hour\n",
    "    dff[\"weekday\"] = dff.timestamp.dt.weekday\n",
    "    dff[\"month\"] = dff.timestamp.dt.month\n",
    "    dff[\"year\"] = dff.timestamp.dt.year \n",
    "    \n",
    "    dff = dff.sort_values(by = \"timestamp\")\n",
    "    dff[\"time_idx\"] = dff.index\n",
    "\n",
    "    dff[\"time_norm\"] = 2 * np.pi * dff[\"time_idx\"] / dff[\"time_idx\"].max()\n",
    "    dff[\"cos_time\"] = np.cos(dff[\"time_norm\"])\n",
    "    dff[\"sin_time\"] = np.sin(dff[\"time_norm\"])\n",
    "\n",
    "    dff[\"month_cos\"] = np.cos(2*np.pi*dff[\"time_idx\"]/(30.4*24))\n",
    "    dff[\"month_sin\"] = np.sin(2*np.pi*dff[\"time_idx\"]/(30.4*24))\n",
    "        \n",
    "    dff[\"weekday_cos\"] = np.cos(2*np.pi*dff[\"time_idx\"]/(7*24))\n",
    "    dff[\"weekday_sin\"] = np.sin(2*np.pi*dff[\"time_idx\"]/(7*24))\n",
    "\n",
    "    return dff.set_index(\"timestamp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = metadata[metadata_cols]\n",
    "weather = weather[weather_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add site_id to electricity:\n",
    "electricity = pd.merge(electricity, metadata, on = \"building_id\", how = \"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "buildings = electricity.building_id.unique()\n",
    "buildings = [building for building in buildings if building not in bad_buildings]\n",
    "electricity = electricity[electricity.building_id.isin(buildings)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sites = electricity.site_id.unique().to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Panther',\n",
       " 'Robin',\n",
       " 'Fox',\n",
       " 'Rat',\n",
       " 'Bear',\n",
       " 'Lamb',\n",
       " 'Peacock',\n",
       " 'Moose',\n",
       " 'Gator',\n",
       " 'Bull',\n",
       " 'Bobcat',\n",
       " 'Crow',\n",
       " 'Wolf',\n",
       " 'Hog',\n",
       " 'Eagle',\n",
       " 'Cockatoo',\n",
       " 'Mouse']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train.parq write complete\n",
      "val.parq write complete\n",
      "Panther complete\n",
      "Robin fail\n",
      "Fox fail\n",
      "train.parq write complete\n",
      "val.parq write complete\n",
      "Rat complete\n",
      "train.parq write complete\n",
      "val.parq write complete\n",
      "Bear complete\n",
      "Lamb fail\n",
      "train.parq write complete\n",
      "val.parq write complete\n",
      "Peacock complete\n",
      "Moose fail\n",
      "train.parq write complete\n",
      "val.parq write complete\n",
      "Gator complete\n",
      "train.parq write complete\n",
      "val.parq write complete\n",
      "Bull complete\n",
      "train.parq write complete\n",
      "val.parq write complete\n",
      "Bobcat complete\n",
      "Crow fail\n",
      "Wolf fail\n",
      "train.parq write complete\n",
      "val.parq write complete\n",
      "Hog complete\n",
      "train.parq write complete\n",
      "val.parq write complete\n",
      "Eagle complete\n",
      "train.parq write complete\n",
      "val.parq write complete\n",
      "Cockatoo complete\n",
      "Mouse fail\n"
     ]
    }
   ],
   "source": [
    "for site in sites:\n",
    "    try:\n",
    "\n",
    "        dfb = []\n",
    "\n",
    "        e = electricity.loc[electricity.site_id == site, :]\n",
    "        w = weather.loc[weather.site_id == site, :]\n",
    "        b = e.building_id.unique()\n",
    "\n",
    "        for building in b:\n",
    "            df = e.loc[e.building_id == building, :]\n",
    "            df = df.merge(w, left_on = [\"timestamp\"], right_on = [\"timestamp\"], how = \"left\")\n",
    "            df = df.sort_values(\"timestamp\").set_index(\"timestamp\")\n",
    "            df[\"cloud_coverage\"] = df[\"cloud_coverage\"].astype(\"category\")\n",
    "            df = df.rename(columns = {\"site_id_x\": \"site_id\"})\n",
    "            df = df.drop(columns= [\"site_id_y\"])\n",
    "            for col in df.columns:\n",
    "                if col == \"cloud_coverage\":\n",
    "                    df.loc[:, col] = df.loc[:, col].fillna(method=\"bfill\") # Back fill this categorical\n",
    "                elif col in [\"site_id\", \"building_id\", \"timestamp\", \"sq_meter\"]:\n",
    "                    pass\n",
    "                else:\n",
    "                    q_low = df[col].quantile(0.01)\n",
    "                    q_hi  = df[col].quantile(0.99)\n",
    "                    df.loc[~((df[col] > q_low) & (df[col] < q_hi)), col] = np.nan\n",
    "                    df.loc[:, col] = df.loc[:, col].interpolate(method = \"cubic\", limit = 6)\n",
    "            df = time_features(df.reset_index())\n",
    "            dfb.append(df)\n",
    "        result = pd.concat(dfb)\n",
    "        result.isna().sum()*100/len(result)\n",
    "\n",
    "        cols = result.columns.to_list()\n",
    "        cols.remove(\"precipitation_depth_6_hr\")\n",
    "        result = result[cols].dropna(how = \"any\")\n",
    "        result = result.sort_values([\"building_id\", \"timestamp\"])\n",
    "        result.isna().sum()*100/len(result)\n",
    "        buildings = result.building_id.unique()\n",
    "        result.building_id = result.building_id.astype(\"category\")\n",
    "        result.cloud_coverage = result.cloud_coverage.astype(\"category\")\n",
    "\n",
    "        days_test = 7\n",
    "        test_lengths = 24 * days_test\n",
    "\n",
    "        dfs_test = []\n",
    "        dfs_train = []\n",
    "        dfs_val = []\n",
    "\n",
    "        for building in buildings:\n",
    "            df = result.loc[result.building_id == building, :].sort_values([\"timestamp\"]).reset_index(drop = False)\n",
    "            val_cutoff = int(df.shape[0] * 0.80)\n",
    "            df_train = df[:val_cutoff]\n",
    "            df_val = df[val_cutoff:]\n",
    "            \n",
    "            dfs_train.append(df_train)\n",
    "            dfs_val.append(df_val)\n",
    "\n",
    "        train = pd.concat(dfs_train).reset_index(drop = True)\n",
    "        val = pd.concat(dfs_val).reset_index(drop = True)\n",
    "\n",
    "        directory = f\"forecasting/data/{site}\"\n",
    "        file_name = f\"test.parq\"\n",
    "\n",
    "        directory = f\"forecasting/data/{site}\"\n",
    "        file_name = f\"train.parq\"\n",
    "        train.to_parquet(path = file_name, engine = \"pyarrow\", compression = \"gzip\", index = False)\n",
    "        storage.upload(file_system, directory = directory, file_name = file_name, file_path = file_name, overwrite=True)\n",
    "\n",
    "        directory = f\"forecasting/data/{site}\"\n",
    "        file_name = f\"val.parq\"\n",
    "        val.to_parquet(path = file_name, engine = \"pyarrow\", compression = \"gzip\", index = False)\n",
    "        storage.upload(file_system, directory = directory, file_name = file_name, file_path = file_name, overwrite=True)\n",
    "        print(f\"{site} complete\")\n",
    "    except:\n",
    "        print(f\"{site} fail\")\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a36958482b636de30c6f22e27dd07f9205d77a571c040a53f5dd2b190cea407e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
