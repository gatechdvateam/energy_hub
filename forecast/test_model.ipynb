{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(\"..\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from webapp.utils.azure_utils import KeyVault, DataLake\n",
    "import dask.dataframe as dd\n",
    "import dask.array as da\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import torch\n",
    "\n",
    "from pytorch_forecasting import Baseline, TemporalFusionTransformer, TimeSeriesDataSet\n",
    "from pytorch_forecasting.data import GroupNormalizer, MultiNormalizer, TorchNormalizer, EncoderNormalizer\n",
    "from pytorch_forecasting.metrics import SMAPE, PoissonLoss, QuantileLoss, MultiLoss, MAE, RMSE\n",
    "from pytorch_forecasting.models.temporal_fusion_transformer.tuning import optimize_hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to Storage Account\n",
    "vault = KeyVault(keyVaultName = \"keyvaultdva2022\")\n",
    "storage_credential = vault.get_secret(secretName = \"storagePrimaryKey\")\n",
    "storage = DataLake(account_name = \"storageaccountdva\", credential = storage_credential)\n",
    "file_system = \"energyhub\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data:\n",
    "metadata = storage.read(file_system, directory = \"data_parq/metadata\", file_name = \"metadata.parq\", extension = \"parq\")\n",
    "weather = storage.read(file_system, directory = \"data_parq/weather\", file_name = \"weather.parq\", extension = \"parq\")\n",
    "electricity = storage.read(file_system, directory = \"data_parq/meters\", file_name = \"electricity.parq\", extension = \"parq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "sites = metadata.site_id.unique()\n",
    "site = sites[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "timestamp                    0.000000\n",
       "site_id                      0.000000\n",
       "air_temperature              0.038651\n",
       "cloud_coverage              51.631810\n",
       "dew_temperature              0.099044\n",
       "precipitation_depth_1_hr    40.217293\n",
       "precipitation_depth_6_hr    94.515741\n",
       "sea_level_pressure           6.529656\n",
       "wind_direction               3.927034\n",
       "wind_speed                   0.173327\n",
       "dtype: float64"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather.isna().sum()*100/len(weather)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "timestamp                    0.000000\n",
       "site_id                      0.000000\n",
       "air_temperature              0.033820\n",
       "cloud_coverage              46.451704\n",
       "dew_temperature              0.086663\n",
       "precipitation_depth_1_hr    38.499838\n",
       "precipitation_depth_6_hr    83.391335\n",
       "sea_level_pressure           6.378485\n",
       "wind_direction               3.436154\n",
       "wind_speed                   0.151661\n",
       "dtype: float64"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int_cols = ['air_temperature', 'cloud_coverage', 'dew_temperature', 'precipitation_depth_1_hr',\n",
    "       'precipitation_depth_6_hr', 'sea_level_pressure', 'wind_direction',\n",
    "       'wind_speed']\n",
    "       \n",
    "df_w_res = []\n",
    "for site in sites:\n",
    "    for col in int_cols:\n",
    "        df_w = weather[weather[\"site_id\"] == site].copy()\n",
    "        df_w[col] = df_w[col].interpolate(method = \"linear\")\n",
    "        df_w_res.append(df_w)\n",
    "weather = pd.concat(df_w_res)\n",
    "\n",
    "weather.isna().sum()*100/len(weather)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add site_id to electricity:\n",
    "e = pd.merge(electricity, metadata[['building_id', 'site_id']], on = \"building_id\", how = \"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use one site only:\n",
    "\n",
    "m = metadata[metadata[\"site_id\"] == site]\n",
    "m = metadata[['building_id', 'site_id', 'sq_meter']]\n",
    "w = weather[weather[\"site_id\"] == site]\n",
    "e = e[e[\"site_id\"] == site]\n",
    "buildings = e.building_id.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Dask dataframe:\n",
    "e = dd.from_pandas(e, npartitions=10)\n",
    "m = dd.from_pandas(m, npartitions=10)\n",
    "w = dd.from_pandas(w, npartitions=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge datasets: \n",
    "df = dd.merge(e, m, on = [\"building_id\", \"site_id\"], how = \"left\")\n",
    "df = dd.merge(df, w, on = [\"site_id\", \"timestamp\"], how = \"left\").head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert ts to datetime and set as index:\n",
    "df[\"timestamp\"] = dd.to_datetime(df[\"timestamp\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert site id and building id to category:\n",
    "df[\"building_id\"] = df[\"building_id\"].astype(\"category\")\n",
    "df[\"site_id\"] = df[\"site_id\"].astype(\"category\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a36958482b636de30c6f22e27dd07f9205d77a571c040a53f5dd2b190cea407e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
